{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64d7fa5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc57611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a73233",
   "metadata": {},
   "source": [
    "### Connecting to ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4041c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login for the notebook running in AGOL\n",
    "#gis = GIS(\"home\")\n",
    "\n",
    "# Login for the notebook running in Pro locally...\n",
    "gis = GIS(\"pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c5b53",
   "metadata": {},
   "source": [
    "### Variables\n",
    "Currently the production layer (which is already powering the very very pretty Dashboard) is commented out and I'm grabbing a test layer instead (next block down). Will un-comment the operational layer when I'm 100% sure things aren't blowing up anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee2370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FeatureLayer url:\"https://services9.arcgis.com/GDVaV4SDJDDBT8gi/arcgis/rest/services/Disaster_Declarations_Summaries_v2/FeatureServer/1\">\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The Item ID of the service containing both the geometry layers and the dashboard layer\n",
    "# dd is \"Disaster Declarations\"\n",
    "dd_id = \"d37c3c2a6f1c4586baad82828bfc3c59\"\n",
    "\n",
    "# Get the item at this item ID\n",
    "dd_item = gis.content.get(dd_id)\n",
    "\n",
    "# Item ID 1 is the input layer used for getting geometries\n",
    "geometries_layer = dd_item.layers[1]\n",
    "\n",
    "print(geometries_layer)\n",
    "\n",
    "# Item ID 0 is the output layer displayed in the dashboard\n",
    "# COMMENTING OUT WHILE TESTING\n",
    "#dashboard_layer = dd_item.layers[0]\n",
    "\n",
    "#print(dashboard_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f835e84",
   "metadata": {},
   "source": [
    "### Get the Dashboard layer FOR TESTING ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707c59ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FeatureLayer url:\"https://services9.arcgis.com/GDVaV4SDJDDBT8gi/arcgis/rest/services/DisasterDeclarations_forTesting_2025only/FeatureServer/0\">\n"
     ]
    }
   ],
   "source": [
    "# ID of the item including Disaster Declarations Summaries subset FOR TESTING ONLY,\n",
    "# replaces \"dashboard_layer\" above through duration of testing\n",
    "test_id = \"edb716da51bc4f7882d13d425ad08fd2\"\n",
    "\n",
    "test_item = gis.content.get(test_id)\n",
    "\n",
    "dashboard_layer = test_item.layers[0]\n",
    "\n",
    "print(dashboard_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b29728",
   "metadata": {},
   "source": [
    "### Build dict of primary/foreign keys\n",
    "I'm thinking I'll need a little dict of the various fields I'll need to comparefor querying the geometries layer...? The key is the field name in the FEMA data; the value is the corresponding field in my geometries layer... ACTUALLY I need to do that backwards because placeCODE would create duplicate keys... SO the key will be in the geometries; the value will be the FEMA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f638e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just updated the values below to reflect the fields I've added to the Summaries df; they should be GTG now\n",
    "key_dict = {\n",
    "    \"State_FIPS\": \"fipsStateCode\",\n",
    "    \"Full_FIPS\": \"fipsFullCode\",\n",
    "    \"AIANNHFP1\": \"fipsTribalCode\",\n",
    "    \"AIANNHFP2\": \"fipsTribalCode\", # No idea why there are three of these...or why the Summaries sometimes use 2 and 3...\n",
    "    \"AIANNHFP3\": \"fipsTribalCode\" # What a pain in the @$$...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f4838",
   "metadata": {},
   "source": [
    "I guess the above block combined with the below block might be a good way to reference the correct fields; it will either just be state_field or key_dict[state_field] to point to either the geometries layer field or the dataframe column, respectively. We'll see if this remains relevant as the script developes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ea439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I believe I will need three different queries for my geometries layer\n",
    "# I'll need to query to get state geometries, county geometries and tribal area geometries:\n",
    "\n",
    "# Get the states on this field:\n",
    "# (States we'll check first; )\n",
    "state_field = \"State_FIPS\"\n",
    "\n",
    "# Get the counties on this field:\n",
    "county_field = \"Full_FIPS\"\n",
    "\n",
    "# Crappily, in assembling the original dashboard layer I realized that\n",
    "# the Declaration data may match ANY ONE of these tribal fields, so I need to check all three:\n",
    "tribal_field1 = \"AIANNHFP1\"\n",
    "tribal_field2 = \"AIANNHFP2\"\n",
    "tribal_field3 = \"AIANNHFP3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d93b45",
   "metadata": {},
   "source": [
    "### Connect to OpenFEMA API and get Disaster Declarations Summaries\n",
    "* Right right right; I forgot that the API by default only returns 1000 records. I shouldn't really NEED more records than that, since the script is going to be run once per day. One thousand records should be MORE than enough. But, I now realize I need to do a little footwork to make sure I am just getting the 1000 _most recent_ records...\n",
    "\n",
    "* Okay added a sort order to the api call to only get the most recent records by declarationDate! That should do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within the API URL, filter the records to return only fyDeclared to 2013 or newer.\n",
    "# US Census GDBs only go back to 2013; before that it's shapefiles only\n",
    "# and I refuse to touch shapefiles, at least for the scope of this project.\n",
    "\n",
    "api_url = r\"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries?$filter=fyDeclared ge 2013&$orderby=declarationDate desc\"\n",
    "\n",
    "# Plug in the URL and capture the response obj\n",
    "response = requests.get(api_url)\n",
    "\n",
    "# Convert response to JSON\n",
    "data = response.json()\n",
    "\n",
    "# Okay so after a little digging I really only need the following (leave out the metadata)\n",
    "summaries_df = pd.DataFrame(data[\"DisasterDeclarationsSummaries\"])\n",
    "\n",
    "summaries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3339283",
   "metadata": {},
   "source": [
    "### Coffee Break 9/3\n",
    "Okay yeah it's a mess in here right now. Early development stages. Pardon the dust.\n",
    "Need to figure out exactly how this whole thing is going to work...\n",
    "...below begin the algorithms...\n",
    "\n",
    "Things that will have to be done, in no particular order:\n",
    "\n",
    "* BEFORE I dissolve (or the pd equivalent of dissolve) the summaries to the FEMA Declaration String level, I need to get all the counties / tribal areas associated with that string so I can grab their geometries and actually perform the spatial dissolve on them\n",
    "* Tricky with the above: If the FEMA Declaration String applies to both counties and tribal areas, these are treated separately\n",
    "* Once I have the list of entities the FEMA Declaration String is for, grab the geometries for those counties and perform the dissolve on them. That new dissolve geometry will be the geometry applied to the new row written to the output Summaries dataset.\n",
    "* THEN dissolve (pd equivalent) the actual summaries, ensuring the schema matches that of the target Summaries dataset...tack on the geometry, apply edits.\n",
    "* Other tricky bits: Obviously we need to check if a given FEMA string already exists within the summaries dataset. I will need to check whether this is as straightforward as it sounds, or whether...hm. I will have to check whether the potential exists for Summary rows with the same FEMA String to be issued across multiple days...e.g., this string for these three counties is issued this day; then another three rows for an additional three counties are added UNDER THE SAME STRING...this will complicate the checking process.\n",
    "* Yet more tricky bits: The fields from the summaries used to match the data to the appropriate geometries is different depending on the level of the entity being matched.\n",
    "\n",
    "For example, Statewide declarations will match on state fips, of course. Counties will match on a combination of state + county code (I could calc that field in the df before I begin...?), and tribal entities will match on...wtf will they match on again...I believe it's a concat of the state FIPS and placeCode...yes because the long tribal codes in the geometries are seven chars...I think. ðŸ¥´\n",
    "\n",
    "I could have just gone easy on myself and committed to representing only county-based declaration rows in my map. But does that simplicity accurately reflect the real world? NO. Does it create a more impressive script? NO! Does it get me a job faster? NO!! ðŸ˜¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5b798",
   "metadata": {},
   "source": [
    "### Convert pseudo-date columns to actual date columns\n",
    "I checked all the fips / code fields to ensure they're object / string type (I deleted that block while tidying up), so the offending fields remaining are the pseudo-date fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9999ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\"declarationDate\", \"incidentBeginDate\", \"incidentEndDate\", \"disasterCloseoutDate\", \"lastIAFilingDate\", \"lastRefresh\"]\n",
    "\n",
    "for dc in date_columns:\n",
    "    summaries_df[dc] = pd.to_datetime(summaries_df[dc], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee860810",
   "metadata": {},
   "source": [
    "### Add full FIPS Code (counties) and full Tribal Code columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bd54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    06009\n",
      "1    30077\n",
      "2    41017\n",
      "3    41031\n",
      "4    15003\n",
      "Name: fipsFullCode, dtype: object\n",
      "0    0699009\n",
      "1    3099077\n",
      "2    4199017\n",
      "3    4199031\n",
      "4    1599003\n",
      "Name: fipsTribalCode, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Add to my df the fields I will need for comparison\n",
    "summaries_df[\"fipsFullCode\"] = summaries_df[\"fipsStateCode\"] + summaries_df[\"fipsCountyCode\"]\n",
    "summaries_df[\"fipsTribalCode\"] = summaries_df[\"fipsStateCode\"] + summaries_df[\"placeCode\"]\n",
    "\n",
    "print(summaries_df[\"fipsFullCode\"].head())\n",
    "print(summaries_df[\"fipsTribalCode\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5ff54",
   "metadata": {},
   "source": [
    "* Considering how I'm going to get the data from the API in shape for the comparison etc. I should just add the two additional columns I added manually for the dashboard I made first, COVID and Entity. After I add and calculate them, the comparisons will all be much easier, because I can just reference those fields for processing the data in chunks (i.e. step 1 process statewide, step 2 process counties, step 3 process tribal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44961960",
   "metadata": {},
   "source": [
    "### Add & calc \"COVID19\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good lord I can't remember how to calculate any of these fields with pandas... ðŸ¤£ðŸ˜­\n",
    "# Anyway the first one I need to calc is the COVID column, simple yes/no\n",
    "\n",
    "summaries_df[\"COVID19\"] = np.where(summaries_df[\"declarationTitle\"].str.contains(\"COVID-19\"), \"Show only COVID-19\", \"Show only non-COVID-19\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ea0ff",
   "metadata": {},
   "source": [
    "### Add & calc \"Entities\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For my next trick I'll use np.select instead of np.where since to code new Entity column\n",
    "# I have three possible values not just 2 / yes no / on off\n",
    "\n",
    "entity_conditions = [\n",
    "    summaries_df[\"designatedArea\"] == \"Statewide\",\n",
    "    (summaries_df[\"designatedArea\"] != \"Statewide\") & (summaries_df[\"fipsCountyCode\"] == \"000\"),\n",
    "    (summaries_df[\"designatedArea\"] != \"Statewide\") & (summaries_df[\"fipsCountyCode\"] != \"000\")\n",
    "]\n",
    "\n",
    "entity_values = [\"State or Equivalent\", \"Tribal Area or Equivalent\", \"County or Equivalent\"]\n",
    "\n",
    "summaries_df[\"Entity\"] = np.select(entity_conditions, entity_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e188aa1",
   "metadata": {},
   "source": [
    "### Coffee Break 9/4\n",
    "Nice! Okay! So far, we've added the full FIPS code (5 digit) and the full tribal code (7-digit). I've also added the COVID field (basically yes or no) and the Entity field (State, County or Tribal). I mean that's good progress for feeling crappy the last few days (not like, sick crappy, just Michigan-is-so-f*cking-lame crappy).\n",
    "\n",
    "Now, what needs to be done next?\n",
    "\n",
    "* I first need to compare the values that are already in the dissolved Summaries layer (the dashboard layer) with what's in my dataframe.\n",
    "* Before I do that, I need to figure out exactly what field we're going to compare on. My first impulse is to use the FEMA Declaration String, but as we discussed above, not sure if that's going to work well. For example...if FEMA pushes out twenty counties with one FEMA Dec String one day, then they decide to push out another ten counties with the same Dec String the next day, if I do the comparison the way I intend, I'll fine that Dec String in my dashboard already and ignore it. I won't get those ten new counties.\n",
    "* I guess I really need to root around in the data to see if my impulse is a valid way to compare the data...\n",
    "\n",
    "Okay so I'll do that in a minute here. What needs to happen then?\n",
    "* I'll need to do the actual comparison, whatever that looks like. Hopefully it's just looking for FEMA Dec Strings in the Dashboard and it's as simple as that.\n",
    "* I'd probably just keep a running tally of the Dec Strings that are already in the dashboard and then just drop those records from my df (technically make a new df where I've dropped those records)\n",
    "* Once I've isolated the new Dec Strings, I can't dissolve them yet; I need to get the geometries of the associated areas.\n",
    "* I get all the geometries associated with a given Dec String, do the dissolve thingy (the actual dissolve gp tool thingy), then...well that's as far as my brain needs to go right now.\n",
    "\n",
    "First let's analyze the temporal spans of all observations with some help with your friend and mine ChatGPT..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871d84d",
   "metadata": {},
   "source": [
    "### Groupby \"femaDeclarationString\", get temporal range (prelim analysis; to delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40e248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = summaries_df.groupby(\"femaDeclarationString\")[\"declarationDate\"].agg([\"min\", \"max\"])\n",
    "\n",
    "analyze[\"range\"] = (analyze[\"max\"] - analyze[\"min\"]).dt.total_seconds() / 3600\n",
    "\n",
    "analyze[\"range\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838fb85",
   "metadata": {},
   "source": [
    "Well that's encouraging (no really I'm not even being sarcastic). I get a whole column of big fat zeros for the declaredDate ranges of all the fema Dec Strings. Honestly...that means I should be fine just proceeding with my simple it's-there-or-not check. I should probably also commit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
