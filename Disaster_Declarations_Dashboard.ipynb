{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64d7fa5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc57611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import manage_data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a73233",
   "metadata": {},
   "source": [
    "### Connecting to ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4041c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login for the notebook running in AGOL\n",
    "#gis = GIS(\"home\")\n",
    "\n",
    "# Login for the notebook running locally...\n",
    "gis = GIS(\"pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c5b53",
   "metadata": {},
   "source": [
    "### Variables\n",
    "Currently the production layer (which is already powering the very very pretty Dashboard) is commented out and I'm grabbing a test layer instead (next block down). Will un-comment the operational layer when I'm 100% sure things aren't blowing up anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee2370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FeatureLayer url:\"https://services9.arcgis.com/GDVaV4SDJDDBT8gi/arcgis/rest/services/Disaster_Declarations_Summaries_v2/FeatureServer/1\">\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The Item ID of the service containing both the geometry layers and the dashboard layer\n",
    "# dd is \"Disaster Declarations\"\n",
    "dd_id = \"d37c3c2a6f1c4586baad82828bfc3c59\"\n",
    "\n",
    "# Get the item at this item ID\n",
    "dd_item = gis.content.get(dd_id)\n",
    "\n",
    "# Item ID 1 is the input layer used for getting geometries\n",
    "geometries_layer = dd_item.layers[1]\n",
    "\n",
    "print(geometries_layer)\n",
    "\n",
    "# Item ID 0 is the output layer displayed in the dashboard\n",
    "# COMMENTING OUT WHILE TESTING\n",
    "#dashboard_layer = dd_item.layers[0]\n",
    "\n",
    "#print(dashboard_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f835e84",
   "metadata": {},
   "source": [
    "### Get the Dashboard layer FOR TESTING ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707c59ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FeatureLayer url:\"https://services9.arcgis.com/GDVaV4SDJDDBT8gi/arcgis/rest/services/DisasterDeclarations_forTesting_2025only/FeatureServer/0\">\n"
     ]
    }
   ],
   "source": [
    "# ID of the item including Disaster Declarations Summaries subset FOR TESTING ONLY,\n",
    "# replaces \"dashboard_layer\" above through duration of testing\n",
    "test_id = \"edb716da51bc4f7882d13d425ad08fd2\"\n",
    "\n",
    "test_item = gis.content.get(test_id)\n",
    "\n",
    "dashboard_layer = test_item.layers[0]\n",
    "\n",
    "print(dashboard_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d93b45",
   "metadata": {},
   "source": [
    "### Connect to OpenFEMA API and get Disaster Declarations Summaries\n",
    "* Right right right; I forgot that the API by default only returns 1000 records. I shouldn't really NEED more records than that, since the script is going to be run once per day. One thousand records should be MORE than enough. But, I now realize I need to do a little footwork to make sure I am just getting the 1000 _most recent_ records...\n",
    "\n",
    "* Okay added a sort order to the api call to only get the most recent records by declarationDate! That should do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d0bef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>femaDeclarationString</th>\n",
       "      <th>disasterNumber</th>\n",
       "      <th>state</th>\n",
       "      <th>declarationType</th>\n",
       "      <th>declarationDate</th>\n",
       "      <th>fyDeclared</th>\n",
       "      <th>incidentType</th>\n",
       "      <th>declarationTitle</th>\n",
       "      <th>ihProgramDeclared</th>\n",
       "      <th>iaProgramDeclared</th>\n",
       "      <th>...</th>\n",
       "      <th>placeCode</th>\n",
       "      <th>designatedArea</th>\n",
       "      <th>declarationRequestNumber</th>\n",
       "      <th>lastIAFilingDate</th>\n",
       "      <th>incidentId</th>\n",
       "      <th>region</th>\n",
       "      <th>designatedIncidentTypes</th>\n",
       "      <th>lastRefresh</th>\n",
       "      <th>hash</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FM-5612-CA</td>\n",
       "      <td>5612</td>\n",
       "      <td>CA</td>\n",
       "      <td>FM</td>\n",
       "      <td>2025-09-03T00:00:00.000Z</td>\n",
       "      <td>2025</td>\n",
       "      <td>Fire</td>\n",
       "      <td>2-7 FIRE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>99009</td>\n",
       "      <td>Calaveras (County)</td>\n",
       "      <td>25121</td>\n",
       "      <td>None</td>\n",
       "      <td>2025090301</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>2025-09-03T18:41:07.857Z</td>\n",
       "      <td>d017531813b75fc753371c26b246931d48de651e</td>\n",
       "      <td>28a1ba9f-d914-4024-9e75-4a66b5bba092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FM-5611-MT</td>\n",
       "      <td>5611</td>\n",
       "      <td>MT</td>\n",
       "      <td>FM</td>\n",
       "      <td>2025-08-26T00:00:00.000Z</td>\n",
       "      <td>2025</td>\n",
       "      <td>Fire</td>\n",
       "      <td>WINDY ROCK FIRE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>99077</td>\n",
       "      <td>Powell (County)</td>\n",
       "      <td>25119</td>\n",
       "      <td>None</td>\n",
       "      <td>2025082701</td>\n",
       "      <td>8</td>\n",
       "      <td>R</td>\n",
       "      <td>2025-08-28T18:01:23.160Z</td>\n",
       "      <td>29e175a73b969da6864182e703e3cb3f8d0bb32d</td>\n",
       "      <td>41329e57-2046-4196-a63d-902f3e7c923c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FM-5610-OR</td>\n",
       "      <td>5610</td>\n",
       "      <td>OR</td>\n",
       "      <td>FM</td>\n",
       "      <td>2025-08-23T00:00:00.000Z</td>\n",
       "      <td>2025</td>\n",
       "      <td>Fire</td>\n",
       "      <td>FLAT FIRE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>99017</td>\n",
       "      <td>Deschutes (County)</td>\n",
       "      <td>25117</td>\n",
       "      <td>None</td>\n",
       "      <td>2025082301</td>\n",
       "      <td>10</td>\n",
       "      <td>R</td>\n",
       "      <td>2025-08-25T18:21:58.453Z</td>\n",
       "      <td>c4a190d030807595da90813aabc6ad2175917668</td>\n",
       "      <td>df7cb24f-8e5a-4c1e-923e-4c75c9ec4581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FM-5610-OR</td>\n",
       "      <td>5610</td>\n",
       "      <td>OR</td>\n",
       "      <td>FM</td>\n",
       "      <td>2025-08-23T00:00:00.000Z</td>\n",
       "      <td>2025</td>\n",
       "      <td>Fire</td>\n",
       "      <td>FLAT FIRE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>99031</td>\n",
       "      <td>Jefferson (County)</td>\n",
       "      <td>25117</td>\n",
       "      <td>None</td>\n",
       "      <td>2025082301</td>\n",
       "      <td>10</td>\n",
       "      <td>R</td>\n",
       "      <td>2025-08-25T18:21:58.453Z</td>\n",
       "      <td>8b07b29243bdbba511790332bd3fa9cca0fe33fd</td>\n",
       "      <td>f0604c05-113b-449e-8e4a-f3b5076af546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FM-5609-HI</td>\n",
       "      <td>5609</td>\n",
       "      <td>HI</td>\n",
       "      <td>FM</td>\n",
       "      <td>2025-08-19T00:00:00.000Z</td>\n",
       "      <td>2025</td>\n",
       "      <td>Fire</td>\n",
       "      <td>KUNIA ROAD FIRE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>99003</td>\n",
       "      <td>Honolulu (County)</td>\n",
       "      <td>25114</td>\n",
       "      <td>None</td>\n",
       "      <td>2025082001</td>\n",
       "      <td>9</td>\n",
       "      <td>R</td>\n",
       "      <td>2025-08-21T18:22:16.374Z</td>\n",
       "      <td>731df26a647e5a0338177f445bab7a23b6f8d6ed</td>\n",
       "      <td>ffab7fa0-2e69-428d-b4d3-da95ca352c03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  femaDeclarationString  disasterNumber state declarationType  \\\n",
       "0            FM-5612-CA            5612    CA              FM   \n",
       "1            FM-5611-MT            5611    MT              FM   \n",
       "2            FM-5610-OR            5610    OR              FM   \n",
       "3            FM-5610-OR            5610    OR              FM   \n",
       "4            FM-5609-HI            5609    HI              FM   \n",
       "\n",
       "            declarationDate  fyDeclared incidentType declarationTitle  \\\n",
       "0  2025-09-03T00:00:00.000Z        2025         Fire         2-7 FIRE   \n",
       "1  2025-08-26T00:00:00.000Z        2025         Fire  WINDY ROCK FIRE   \n",
       "2  2025-08-23T00:00:00.000Z        2025         Fire       FLAT FIRE    \n",
       "3  2025-08-23T00:00:00.000Z        2025         Fire       FLAT FIRE    \n",
       "4  2025-08-19T00:00:00.000Z        2025         Fire  KUNIA ROAD FIRE   \n",
       "\n",
       "   ihProgramDeclared  iaProgramDeclared  ...  placeCode      designatedArea  \\\n",
       "0              False              False  ...      99009  Calaveras (County)   \n",
       "1              False              False  ...      99077     Powell (County)   \n",
       "2              False              False  ...      99017  Deschutes (County)   \n",
       "3              False              False  ...      99031  Jefferson (County)   \n",
       "4              False              False  ...      99003   Honolulu (County)   \n",
       "\n",
       "  declarationRequestNumber lastIAFilingDate  incidentId  region  \\\n",
       "0                    25121             None  2025090301       9   \n",
       "1                    25119             None  2025082701       8   \n",
       "2                    25117             None  2025082301      10   \n",
       "3                    25117             None  2025082301      10   \n",
       "4                    25114             None  2025082001       9   \n",
       "\n",
       "  designatedIncidentTypes               lastRefresh  \\\n",
       "0                       R  2025-09-03T18:41:07.857Z   \n",
       "1                       R  2025-08-28T18:01:23.160Z   \n",
       "2                       R  2025-08-25T18:21:58.453Z   \n",
       "3                       R  2025-08-25T18:21:58.453Z   \n",
       "4                       R  2025-08-21T18:22:16.374Z   \n",
       "\n",
       "                                       hash  \\\n",
       "0  d017531813b75fc753371c26b246931d48de651e   \n",
       "1  29e175a73b969da6864182e703e3cb3f8d0bb32d   \n",
       "2  c4a190d030807595da90813aabc6ad2175917668   \n",
       "3  8b07b29243bdbba511790332bd3fa9cca0fe33fd   \n",
       "4  731df26a647e5a0338177f445bab7a23b6f8d6ed   \n",
       "\n",
       "                                     id  \n",
       "0  28a1ba9f-d914-4024-9e75-4a66b5bba092  \n",
       "1  41329e57-2046-4196-a63d-902f3e7c923c  \n",
       "2  df7cb24f-8e5a-4c1e-923e-4c75c9ec4581  \n",
       "3  f0604c05-113b-449e-8e4a-f3b5076af546  \n",
       "4  ffab7fa0-2e69-428d-b4d3-da95ca352c03  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Within the API URL, filter the records to return only fyDeclared to 2013 or newer.\n",
    "# US Census GDBs only go back to 2013; before that it's shapefiles only\n",
    "# and I refuse to touch shapefiles, at least for the scope of this project.\n",
    "\n",
    "api_url = r\"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries?$filter=fyDeclared ge 2013&$orderby=declarationDate desc\"\n",
    "\n",
    "# Plug in the URL and capture the response obj\n",
    "response = requests.get(api_url)\n",
    "\n",
    "# Convert response to JSON\n",
    "data = response.json()\n",
    "\n",
    "# Okay so after a little digging I really only need the following (leave out the metadata)\n",
    "summaries_df = pd.DataFrame(data[\"DisasterDeclarationsSummaries\"])\n",
    "\n",
    "summaries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5b798",
   "metadata": {},
   "source": [
    "### Convert pseudo-date columns to actual date columns\n",
    "I checked all the fips / code fields to ensure they're object / string type (I deleted that block while tidying up), so the offending fields remaining are the pseudo-date fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9999ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\"declarationDate\", \"incidentBeginDate\", \"incidentEndDate\", \"disasterCloseoutDate\", \"lastIAFilingDate\", \"lastRefresh\"]\n",
    "\n",
    "for dc in date_columns:\n",
    "    summaries_df[dc] = pd.to_datetime(summaries_df[dc], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee860810",
   "metadata": {},
   "source": [
    "### Add full FIPS Code (counties) and full Tribal Code columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31bd54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    06009\n",
      "1    30077\n",
      "2    41017\n",
      "3    41031\n",
      "4    15003\n",
      "Name: fipsFullCode, dtype: object\n",
      "0    0699009\n",
      "1    3099077\n",
      "2    4199017\n",
      "3    4199031\n",
      "4    1599003\n",
      "Name: fipsTribalCode, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Add to my df the fields I will need for comparison\n",
    "summaries_df[\"fipsFullCode\"] = summaries_df[\"fipsStateCode\"] + summaries_df[\"fipsCountyCode\"]\n",
    "summaries_df[\"fipsTribalCode\"] = summaries_df[\"fipsStateCode\"] + summaries_df[\"placeCode\"]\n",
    "\n",
    "print(summaries_df[\"fipsFullCode\"].head())\n",
    "print(summaries_df[\"fipsTribalCode\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5ff54",
   "metadata": {},
   "source": [
    "* Considering how I'm going to get the data from the API in shape for the comparison etc. I should just add the two additional columns I added manually for the dashboard I made first, COVID and Entity. After I add and calculate them, the comparisons will all be much easier, because I can just reference those fields for processing the data in chunks (i.e. step 1 process statewide, step 2 process counties, step 3 process tribal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44961960",
   "metadata": {},
   "source": [
    "### Add & calc \"COVID19\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f76cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good lord I can't remember how to calculate any of these fields with pandas... ðŸ¤£ðŸ˜­\n",
    "# Anyway the first one I need to calc is the COVID column, simple yes/no\n",
    "\n",
    "summaries_df[\"COVID19\"] = np.where(summaries_df[\"declarationTitle\"].str.contains(\"COVID-19\"), \"Show only COVID-19\", \"Show only non-COVID-19\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ea0ff",
   "metadata": {},
   "source": [
    "### Add & calc \"Entities\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8262e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For my next trick I'll use np.select instead of np.where since to code new Entity column\n",
    "# I have three possible values not just 2 / yes no / on off\n",
    "\n",
    "entity_conditions = [\n",
    "    summaries_df[\"designatedArea\"] == \"Statewide\",\n",
    "    (summaries_df[\"designatedArea\"] != \"Statewide\") & (summaries_df[\"fipsCountyCode\"] == \"000\"),\n",
    "    (summaries_df[\"designatedArea\"] != \"Statewide\") & (summaries_df[\"fipsCountyCode\"] != \"000\")\n",
    "]\n",
    "\n",
    "entity_values = [\"State or Equivalent\", \"Tribal Area or Equivalent\", \"County or Equivalent\"]\n",
    "\n",
    "summaries_df[\"Entity\"] = np.select(entity_conditions, entity_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871d84d",
   "metadata": {},
   "source": [
    "### Groupby \"femaDeclarationString\", get temporal range (prelim analysis; to delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba40e248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = summaries_df.groupby(\"femaDeclarationString\")[\"declarationDate\"].agg([\"min\", \"max\"])\n",
    "\n",
    "analyze[\"range\"] = (analyze[\"max\"] - analyze[\"min\"]).dt.total_seconds() / 3600\n",
    "\n",
    "analyze[\"range\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399d2d6",
   "metadata": {},
   "source": [
    "### Let's just re-do the bottom four blocks with a one-liner now that ChatGPT is helping jog my memory from DSTP XD\n",
    "Can you believe I forgot about isin?? And MERGE??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6929c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_sdf = dashboard_layer.query().sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e5b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adds_df = summaries_df[~summaries_df[\"femaDeclarationString\"].isin(dashboard_sdf[\"femaDeclarationString\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5e2b4",
   "metadata": {},
   "source": [
    "### Build dict of primary/foreign keys\n",
    "Okay now that I have a little more of the script fleshed out, tidying this up and re-doing a few comments here. Main thing I have to remember for further down the script is that THE KEYS ARE THE FIELDS IN THE GEOMETRIES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "304d81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I believe I will need three different queries for my geometries layer\n",
    "# I'll need to query to get state geometries, county geometries and tribal area geometries:\n",
    "\n",
    "# Get the states on this field:\n",
    "# (States we'll check first; )\n",
    "state_field = \"State_FIPS\"\n",
    "\n",
    "# Get the counties on this field:\n",
    "county_field = \"Full_FIPS\"\n",
    "\n",
    "# Crappily, in assembling the original dashboard layer I realized that\n",
    "# the Declaration data may match ANY ONE of these tribal fields, so I need to check all three:\n",
    "tribal_field1 = \"AIANNHFP1\"\n",
    "tribal_field2 = \"AIANNHFP2\"\n",
    "tribal_field3 = \"AIANNHFP3\"\n",
    "\n",
    "# Just updated the values below to reflect the fields I've added to the Summaries df; they should be GTG now\n",
    "key_fields_dict = {\n",
    "    \"State_FIPS\": \"fipsStateCode\",\n",
    "    \"Full_FIPS\": \"fipsFullCode\",\n",
    "    \"AIANNHFP1\": \"fipsTribalCode\",\n",
    "    \"AIANNHFP2\": \"fipsTribalCode\", # No idea why there are three of these...or why the Summaries sometimes use 2 and 3...\n",
    "    \"AIANNHFP3\": \"fipsTribalCode\" # What a pain in the @$$...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a493b8",
   "metadata": {},
   "source": [
    "### Get geometries associated with each Dec string for all three entity types\n",
    "Since this has to be done technically 5 times (there are three different keys for Tribal), a function it is\n",
    "(Yeah we'll throw everything in a function later, probably, since it's just tidy, but this requires it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d433335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spatial dataframe from the geometries layer\n",
    "# I really only need the \n",
    "geometries_sdf = geometries_layer.query().sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1c85969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [femaDeclarationString, disasterNumber, state, declarationType, declarationDate, fyDeclared, incidentType, declarationTitle, ihProgramDeclared, iaProgramDeclared, paProgramDeclared, hmProgramDeclared, incidentBeginDate, incidentEndDate, disasterCloseoutDate, tribalRequest, fipsStateCode, fipsCountyCode, placeCode, designatedArea, declarationRequestNumber, lastIAFilingDate, incidentId, region, designatedIncidentTypes, lastRefresh, hash, id, fipsFullCode, fipsTribalCode, COVID19, Entity, OBJECTID, Entity_2, Full_FIPS, County_FIPS, County_Name, State_FIPS, State_Name, State_Abbreviation, Tribal_FIPS, Tribal_Basename, Tribal_Name, AIANNHFP1, AIANNHFP2, AIANNHFP3, AIANNHNS, Census_Year, Shape__Area, Shape__Length, SHAPE]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 51 columns]\n",
      "  femaDeclarationString  disasterNumber state declarationType  \\\n",
      "0            FM-5612-CA            5612    CA              FM   \n",
      "1            FM-5611-MT            5611    MT              FM   \n",
      "2            FM-5610-OR            5610    OR              FM   \n",
      "3            FM-5610-OR            5610    OR              FM   \n",
      "\n",
      "            declarationDate  fyDeclared incidentType declarationTitle  \\\n",
      "0 2025-09-03 00:00:00+00:00        2025         Fire         2-7 FIRE   \n",
      "1 2025-08-26 00:00:00+00:00        2025         Fire  WINDY ROCK FIRE   \n",
      "2 2025-08-23 00:00:00+00:00        2025         Fire       FLAT FIRE    \n",
      "3 2025-08-23 00:00:00+00:00        2025         Fire       FLAT FIRE    \n",
      "\n",
      "   ihProgramDeclared  iaProgramDeclared  ...  Tribal_Basename  Tribal_Name  \\\n",
      "0              False              False  ...             <NA>         <NA>   \n",
      "1              False              False  ...             <NA>         <NA>   \n",
      "2              False              False  ...             <NA>         <NA>   \n",
      "3              False              False  ...             <NA>         <NA>   \n",
      "\n",
      "  AIANNHFP1 AIANNHFP2 AIANNHFP3  AIANNHNS Census_Year         Shape__Area  \\\n",
      "0      <NA>      <NA>      <NA>      <NA>        2025   4347170942.960938   \n",
      "1      <NA>      <NA>      <NA>      <NA>        2025  12916839477.203125   \n",
      "2      <NA>      <NA>      <NA>      <NA>        2025  15241972379.453125   \n",
      "3      <NA>      <NA>      <NA>      <NA>        2025   9164136488.898438   \n",
      "\n",
      "   Shape__Length                                              SHAPE  \n",
      "0  319087.863892  {\"rings\": [[[-13427073.7835, 4630770.9105], [-...  \n",
      "1  671747.744721  {\"rings\": [[[-12631038.7077, 6037032.6359], [-...  \n",
      "2  706417.816816  {\"rings\": [[[-13481100.5781, 5486843.1441], [-...  \n",
      "3  466107.542188  {\"rings\": [[[-13513910.2275, 5526480.1649], [-...  \n",
      "\n",
      "[4 rows x 51 columns]\n",
      "Empty DataFrame\n",
      "Columns: [femaDeclarationString, disasterNumber, state, declarationType, declarationDate, fyDeclared, incidentType, declarationTitle, ihProgramDeclared, iaProgramDeclared, paProgramDeclared, hmProgramDeclared, incidentBeginDate, incidentEndDate, disasterCloseoutDate, tribalRequest, fipsStateCode, fipsCountyCode, placeCode, designatedArea, declarationRequestNumber, lastIAFilingDate, incidentId, region, designatedIncidentTypes, lastRefresh, hash, id, fipsFullCode, fipsTribalCode, COVID19, Entity, OBJECTID, Entity_2, Full_FIPS, County_FIPS, County_Name, State_FIPS, State_Name, State_Abbreviation, Tribal_FIPS, Tribal_Basename, Tribal_Name, AIANNHFP1, AIANNHFP2, AIANNHFP3, AIANNHNS, Census_Year, Shape__Area, Shape__Length, SHAPE]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 51 columns]\n",
      "Empty DataFrame\n",
      "Columns: [femaDeclarationString, disasterNumber, state, declarationType, declarationDate, fyDeclared, incidentType, declarationTitle, ihProgramDeclared, iaProgramDeclared, paProgramDeclared, hmProgramDeclared, incidentBeginDate, incidentEndDate, disasterCloseoutDate, tribalRequest, fipsStateCode, fipsCountyCode, placeCode, designatedArea, declarationRequestNumber, lastIAFilingDate, incidentId, region, designatedIncidentTypes, lastRefresh, hash, id, fipsFullCode, fipsTribalCode, COVID19, Entity, OBJECTID, Entity_2, Full_FIPS, County_FIPS, County_Name, State_FIPS, State_Name, State_Abbreviation, Tribal_FIPS, Tribal_Basename, Tribal_Name, AIANNHFP1, AIANNHFP2, AIANNHFP3, AIANNHNS, Census_Year, Shape__Area, Shape__Length, SHAPE]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 51 columns]\n",
      "Empty DataFrame\n",
      "Columns: [femaDeclarationString, disasterNumber, state, declarationType, declarationDate, fyDeclared, incidentType, declarationTitle, ihProgramDeclared, iaProgramDeclared, paProgramDeclared, hmProgramDeclared, incidentBeginDate, incidentEndDate, disasterCloseoutDate, tribalRequest, fipsStateCode, fipsCountyCode, placeCode, designatedArea, declarationRequestNumber, lastIAFilingDate, incidentId, region, designatedIncidentTypes, lastRefresh, hash, id, fipsFullCode, fipsTribalCode, COVID19, Entity, OBJECTID, Entity_2, Full_FIPS, County_FIPS, County_Name, State_FIPS, State_Name, State_Abbreviation, Tribal_FIPS, Tribal_Basename, Tribal_Name, AIANNHFP1, AIANNHFP2, AIANNHFP3, AIANNHNS, Census_Year, Shape__Area, Shape__Length, SHAPE]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "state = \"State or Equivalent\"\n",
    "county = \"County or Equivalent\"\n",
    "tribal = \"Tribal or Equivalent\" \n",
    "\n",
    "def get_geometries(entity, field):\n",
    "\n",
    "    # For the loop I want to make sure I'm only looking at the adds\n",
    "    # that correspond to the entity level I'm interested in here\n",
    "    entity_adds_df = adds_df[adds_df[\"Entity\"] == entity]\n",
    "\n",
    "    # NEED to specify an empty string for at least one of the suffixes here;\n",
    "    # Otherwise I have no \"Entity\" field and my Dashboard layer and adds feature collection won't have matching fields\n",
    "    merged = entity_adds_df.merge(geometries_sdf, left_on=[key_fields_dict[field]], right_on=field, suffixes=('', '_2'))\n",
    "\n",
    "    print(merged.head())\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "state_geometries = get_geometries(state, state_field)\n",
    "\n",
    "county_geometries = get_geometries(county, county_field)\n",
    "\n",
    "tribal1_geometries = get_geometries(tribal, tribal_field1)\n",
    "\n",
    "tribal2_geometries = get_geometries(tribal, tribal_field2)\n",
    "\n",
    "tribal3_geometries = get_geometries(tribal, tribal_field3)\n",
    "\n",
    "# I only need to call for tribal three times since the foreign key is potentially in one of three fields.\n",
    "# But they are all the same type of entity and after the merge the schemas should be identical, so just stack them.\n",
    "tribal_geometries = pd.concat([tribal1_geometries, tribal2_geometries, tribal3_geometries])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d669c68",
   "metadata": {},
   "source": [
    "### Good morning (whoops, afternoon) 9/7\n",
    "Just inserting myself here on a grossly sunny Sunday. Need to wrap the rest of this stuff in functions and slam in ALL the entities, not just counties.\n",
    "\n",
    "But first--can't I--shouldn't I--just smash all the tribal dfs together into one df? There's no reason not to do that right, and I wouldn't want Dec strings that applied to different tribal fields (1, 2, 3) to be added as separate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c71c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_feature_collection(input_df):\n",
    "\n",
    "    if not input_df.empty:\n",
    "        feature_collection = input_df.spatial.to_feature_collection()\n",
    "    else: feature_collection = False\n",
    "\n",
    "    return feature_collection\n",
    "\n",
    "state_adds_fcol = convert_to_feature_collection(state_geometries)\n",
    "\n",
    "county_adds_fcol = convert_to_feature_collection(county_geometries)\n",
    "\n",
    "tribal_adds_fcol = convert_to_feature_collection(tribal_geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "335281f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entity', 'femaDeclarationString', 'disasterNumber', 'state', 'declarationType', 'declarationDate', 'fyDeclared', 'incidentType', 'declarationTitle', 'incidentBeginDate', 'incidentEndDate', 'disasterCloseoutDate', 'tribalRequest', 'fipsStateCode', 'declarationRequestNumber', 'lastIAFilingDate', 'incidentId', 'region', 'designatedIncidentTypes', 'COVID19']\n"
     ]
    }
   ],
   "source": [
    "remove_fields = [\"OBJECTID\", \"Shape__Area\", \"Shape__Length\", \"SHAPE\"]\n",
    "\n",
    "dissolve_fields = [f[\"name\"] for f in dashboard_layer.properties.fields if not f[\"name\"] in remove_fields]\n",
    "\n",
    "print(dissolve_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f363ab",
   "metadata": {},
   "source": [
    "Ah hah! Okay that's why my dissolve on all the fields is failing...in my adds, because of the merge, I have and Entity_x and Entity_y. But no Entity.\n",
    "\n",
    "Soooo I'm pretty sure I can specify the names of those fields right? I remember doing that in DSTP...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dd853fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"cost\": 0.004}\n"
     ]
    }
   ],
   "source": [
    "def disssolve_boundaries(feature_collection):\n",
    "\n",
    "    if feature_collection:\n",
    "        adds_dissolved = manage_data.dissolve_boundaries(feature_collection, dissolve_fields=dissolve_fields)\n",
    "    else: adds_dissolved = False\n",
    "\n",
    "    return adds_dissolved\n",
    "\n",
    "state_adds_dissolved = disssolve_boundaries(state_adds_fcol)\n",
    "\n",
    "county_adds_dissolved = disssolve_boundaries(county_adds_fcol)\n",
    "\n",
    "tribal_adds_dissolved = disssolve_boundaries(tribal_adds_fcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c498b26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'addResults': [{'objectId': 103, 'uniqueId': 103, 'globalId': None, 'success': True}, {'objectId': 104, 'uniqueId': 104, 'globalId': None, 'success': True}, {'objectId': 105, 'uniqueId': 105, 'globalId': None, 'success': True}], 'updateResults': [], 'deleteResults': []}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def append_features(dissolved):\n",
    "\n",
    "    if dissolved:\n",
    "\n",
    "        features = dissolved.query().features\n",
    "\n",
    "        message = dashboard_layer.edit_features(features)\n",
    "\n",
    "        return message\n",
    "\n",
    "state_message = append_features(state_adds_dissolved)\n",
    "\n",
    "county_message = append_features(county_adds_dissolved)\n",
    "\n",
    "tribal_message = append_features(tribal_adds_dissolved)\n",
    "\n",
    "print(state_message)\n",
    "\n",
    "print(county_message)\n",
    "\n",
    "print(tribal_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
